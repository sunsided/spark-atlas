{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PySpark vs. MongoDB Connector for Spark\n",
    "\n",
    " The `pyspark` package and the [MongoDB Connector for Spark](https://www.mongodb.com/docs/spark-connector/v10.2/) are pre-installed in the Docker image."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b24136b64fb2cc24"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T21:46:27.403695547Z",
     "start_time": "2023-10-01T21:46:27.333540557Z"
    }
   },
   "id": "acfde401e35b7735"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Environment Variables from .env file\n",
    "\n",
    "We'll later pick connection strings from there. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c0a997a49e43684"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'mongodb+srv://'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install python-dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# e.g. \"mongodb+srv://<username>:<password>@cluster0.mongodb.net/database.collection\"\n",
    "MONGO_INPUT_URI = os.getenv('MONGO_INPUT_URI', '')\n",
    "MONGO_INPUT_URI[0:14]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T21:55:07.544815419Z",
     "start_time": "2023-10-01T21:55:07.508808831Z"
    }
   },
   "id": "a6f37806fe14c4bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a Spark session"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e6e83e4a7fbd7e6"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# SparkSession.builder.master(\"spark://spark-master:7077\").getOrCreate().stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T21:46:51.396513025Z",
     "start_time": "2023-10-01T21:46:51.371550296Z"
    }
   },
   "id": "22c1f416f3bf4f44"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.4\n"
     ]
    }
   ],
   "source": [
    "# Define Spark session\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"Python Spark SQL MongoDB Atlas example\")\n",
    "    .master('spark://spark-master:7077')\n",
    "    # Announcing the connector JAR is not required in our case because it is\n",
    "    # already bundled with the Dockerfile.\n",
    "    # .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:10.2.0'),\n",
    "    .getOrCreate())\n",
    "\n",
    "print(spark.version)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T21:55:09.733900529Z",
     "start_time": "2023-10-01T21:55:09.727021890Z"
    }
   },
   "id": "db8ce08d34b18552"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                 md5|                 rid|              sha256|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[D2 2C 18 18 55 8...|[41 82 60 9E F3 E...|[26 6F 75 A0 01 6...|\n",
      "|[06 2C BF 06 88 F...|[58 04 F3 E1 A9 5...|[13 40 43 DA 51 C...|\n",
      "|[0C 10 AE 2B B3 1...|[8B B4 68 0E 21 7...|[4A 5E 4E 73 12 0...|\n",
      "|[CC FF CB 2E 37 F...|[60 3D D7 1E 26 2...|[8F 14 E7 A8 69 9...|\n",
      "|[67 12 B9 65 FE F...|[A4 A4 E8 71 F9 9...|[AF C4 C1 36 6B E...|\n",
      "|[0C 10 AE 2B B3 1...|[9A 78 F7 5C E6 E...|[4A 5E 4E 73 12 0...|\n",
      "|[6D 74 35 34 29 F...|[9A 5A 1A 4C 93 4...|[D4 11 8E 7B 3B 7...|\n",
      "|[34 72 34 84 36 7...|[4B 90 FE 53 C0 E...|[3A DC 26 F3 17 E...|\n",
      "|[84 31 B6 CB 68 3...|[00 F7 5A E9 0D 8...|[A8 23 5F B3 6C E...|\n",
      "|[1A C2 10 0B 88 A...|[12 23 54 79 19 2...|[8E C2 9E 1B 9E D...|\n",
      "+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    { '$match': { 'data._t': 'image_info' } },\n",
    "    { '$limit': 10 },\n",
    "    { '$project': { \n",
    "        '_id': 0, \n",
    "        'rid': '$rid',\n",
    "        'sha256': '$data.hashes.sha256',\n",
    "        'md5': '$data.hashes.md5'}\n",
    "    }\n",
    "    ]\n",
    "\n",
    "df = (spark.read.format('mongodb')\n",
    "    .option('connection.uri', MONGO_INPUT_URI)\n",
    "    .option('database', 'request_insights')\n",
    "    .option('collection', 'requests')\n",
    "    .option('aggregation.pipeline', pipeline)\n",
    "    .option('outputExtendedJson', 'true')\n",
    "    .load())\n",
    "\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T22:03:18.269709597Z",
     "start_time": "2023-10-01T22:03:16.516647387Z"
    }
   },
   "id": "5968e54beba0a53"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T21:44:31.723748995Z",
     "start_time": "2023-09-28T21:44:31.289236928Z"
    }
   },
   "id": "1de3730dd0f468a1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
