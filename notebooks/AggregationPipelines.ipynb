{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PySpark vs. MongoDB Connector for Spark\n",
    "\n",
    " The `pyspark` package and the [MongoDB Connector for Spark](https://www.mongodb.com/docs/spark-connector/v10.2/) are pre-installed in the Docker image."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b24136b64fb2cc24"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T07:56:19.666079682Z",
     "start_time": "2023-10-02T07:56:19.576649460Z"
    }
   },
   "id": "acfde401e35b7735"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Environment Variables from .env file\n",
    "\n",
    "We'll later pick connection strings from there. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c0a997a49e43684"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongodb+srv://\n",
      "jdbc:singlestore://\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# e.g. \"mongodb+srv://<username>:<password>@cluster0.mongodb.net/database.collection\"\n",
    "MONGO_INPUT_URI = os.getenv('MONGO_INPUT_URI', '')\n",
    "print(MONGO_INPUT_URI[0:14])\n",
    "\n",
    "# e.g. jdbc:singlestore://host.docker.internal:3036/<database>?user=<username>&password=<password>\n",
    "SINGLESTORE_JDBC_URI = os.getenv('SINGLESTORE_JDBC_URI', '')\n",
    "print(SINGLESTORE_JDBC_URI[0:19])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T07:56:19.676695277Z",
     "start_time": "2023-10-02T07:56:19.670567786Z"
    }
   },
   "id": "a6f37806fe14c4bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ensure the Docker host is reachable by name."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6db98f6997385f4c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'172.17.0.1'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import socket\n",
    "socket.gethostbyname('host.docker.internal')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T07:57:20.911630994Z",
     "start_time": "2023-10-02T07:57:20.859212959Z"
    }
   },
   "id": "a4df058fc17fbe1b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make the port-forward accessible from Docker, ensure it binds to all IPs (or at the very least to the Docker Host IP), e.g.:\n",
    "\n",
    "```shell\n",
    "kubectl port-forward -n singlestore svc/singlestore-ddl --address=0.0.0.0 3306:3306\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1b980a16a780043"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a Spark session"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e6e83e4a7fbd7e6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# SparkSession.builder.master(\"spark://spark-master:7077\").getOrCreate().stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T07:56:19.680830596Z",
     "start_time": "2023-10-02T07:56:19.678384408Z"
    }
   },
   "id": "22c1f416f3bf4f44"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-02 07:56:21 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "# Define Spark session\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"Python Spark SQL MongoDB Atlas example\")\n",
    "    .master('spark://spark-master:7077')\n",
    "    # Announcing the connector JAR is not required in our case because it is\n",
    "    # already bundled with the Dockerfile.\n",
    "    # .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:10.2.0'),\n",
    "    .getOrCreate())\n",
    "\n",
    "print(spark.version)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T07:56:22.919055414Z",
     "start_time": "2023-10-02T07:56:19.683247843Z"
    }
   },
   "id": "db8ce08d34b18552"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run an aggregation against MongoDB Atlas"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2213e10a3fbfa599"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                 md5|                 rid|              sha256|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[D2 2C 18 18 55 8...|[41 82 60 9E F3 E...|[26 6F 75 A0 01 6...|\n",
      "|[06 2C BF 06 88 F...|[58 04 F3 E1 A9 5...|[13 40 43 DA 51 C...|\n",
      "|[0C 10 AE 2B B3 1...|[8B B4 68 0E 21 7...|[4A 5E 4E 73 12 0...|\n",
      "|[CC FF CB 2E 37 F...|[60 3D D7 1E 26 2...|[8F 14 E7 A8 69 9...|\n",
      "|[67 12 B9 65 FE F...|[A4 A4 E8 71 F9 9...|[AF C4 C1 36 6B E...|\n",
      "|[0C 10 AE 2B B3 1...|[9A 78 F7 5C E6 E...|[4A 5E 4E 73 12 0...|\n",
      "|[6D 74 35 34 29 F...|[9A 5A 1A 4C 93 4...|[D4 11 8E 7B 3B 7...|\n",
      "|[34 72 34 84 36 7...|[4B 90 FE 53 C0 E...|[3A DC 26 F3 17 E...|\n",
      "|[84 31 B6 CB 68 3...|[00 F7 5A E9 0D 8...|[A8 23 5F B3 6C E...|\n",
      "|[1A C2 10 0B 88 A...|[12 23 54 79 19 2...|[8E C2 9E 1B 9E D...|\n",
      "+--------------------+--------------------+--------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    { '$match': { 'data._t': 'image_info' } },\n",
    "    { '$limit': 10 },\n",
    "    { '$project': { \n",
    "        '_id': 0, \n",
    "        'rid': '$rid',\n",
    "        'sha256': '$data.hashes.sha256',\n",
    "        'md5': '$data.hashes.md5'}\n",
    "    }\n",
    "    ]\n",
    "\n",
    "df = (spark.read.format('mongodb')\n",
    "    .option('connection.uri', MONGO_INPUT_URI)\n",
    "    .option('database', 'request_insights')\n",
    "    .option('collection', 'requests')\n",
    "    .option('aggregation.pipeline', pipeline)\n",
    "    .option('outputExtendedJson', 'true')\n",
    "    .load())\n",
    "\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T07:56:31.163928739Z",
     "start_time": "2023-10-02T07:56:22.921219637Z"
    }
   },
   "id": "5968e54beba0a53"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run a query against SingleStore (MemSQL)\n",
    "\n",
    "Note that, again, the JDBC driver is bundled in the Docker image."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51994f0d81969fd"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                 (0 + 1) / 1][Stage 23:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                  id|        image_sha256|\n",
      "+--------------------+--------------------+\n",
      "|[69 6E 74 65 72 6...|[69 6D 61 67 65 5...|\n",
      "|[69 6E 74 65 72 6...|[69 6D 61 67 65 5...|\n",
      "|[69 6E 74 65 72 6...|[69 6D 61 67 65 5...|\n",
      "|[69 6E 74 65 72 6...|[69 6D 61 67 65 5...|\n",
      "|[69 6E 74 65 72 6...|[69 6D 61 67 65 5...|\n",
      "|[69 6E 74 65 72 6...|[69 6D 61 67 65 5...|\n",
      "|[69 6E 74 65 72 6...|[69 6D 61 67 65 5...|\n",
      "|[69 6E 74 65 72 6...|[69 6D 61 67 65 5...|\n",
      "|[69 6E 74 65 72 6...|[69 6D 61 67 65 5...|\n",
      "|[69 6E 74 65 72 6...|[69 6D 61 67 65 5...|\n",
      "+--------------------+--------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_t = (spark.read.format(\"jdbc\")\n",
    "      .option(\"url\", SINGLESTORE_JDBC_URI)\n",
    "      # .option(\"dbtable\", \"images_sha256_v2.image_sha256_translation\")\n",
    "      .option(\"query\", \"SELECT (internal_id :> BLOB) AS internal_id, image_sha256 FROM image_sha256_translation LIMIT 10\")\n",
    "      .load())\n",
    "\n",
    "df_t = df_t.withColumnRenamed('internal_id', 'id')\n",
    "df_t.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T09:13:37.028761614Z",
     "start_time": "2023-10-02T09:11:32.569147686Z"
    }
   },
   "id": "30e9407b3d4516a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-02T07:56:31.790020947Z"
    }
   },
   "id": "1de3730dd0f468a1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
