# https://github.com/big-data-europe/docker-spark
version: "3.6"

services:
  jupyterlab:
    image: jupyter/pyspark-notebook:spark-3.5.0
    container_name: jupyterlab
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - GRANT_SUDO=yes
    ports:
      - "8888:8888"
    volumes:
      - shared-workspace:/opt/workspace

  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8090:8080"
      - "7077:7077"
    volumes:
      - shared-workspace:/opt/workspace

  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-1
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=4g
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - "8081:8081"
    volumes:
      - shared-workspace:/opt/workspace
    depends_on:
      - spark-master

  spark-worker-2:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-2
    environment:
      - SPARK_WORKER_CORES=4
      - SPARK_WORKER_MEMORY=4g
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - "8082:8081"
    volumes:
      - shared-workspace:/opt/workspace
    depends_on:
      - spark-master

  spark-history-server:
    image: bde2020/spark-history-server:3.3.0-hadoop3.3
    container_name: spark-history-server
    depends_on:
      - spark-master
    ports:
      - "18081:18081"
    volumes:
      - /tmp/spark-events-local:/tmp/spark-events

volumes:
  shared-workspace:
    name: "hadoop-distributed-file-system"
    driver: local
